Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 集成学习 Ensemble Learning

当数据集有问题，或者网络学习能力不足，或精度不够时，我们可以采取集成学习的方法，来提升性能。说得通俗一些，就是发挥团队的智慧，根据团队中不同背景、不同能力的成员的独立意见，通过某种决策方法来解决一个问题。所以集成学习也称为多分类器系统(multi_classifier system)、基于委员会的学习(committee-based learning)等。

一个简单的集成学习的示意图如下：

<img src=".\Images\16\ensemble.png">

图中有两个组件：

- Individual Learner 个体学习器

如果所有的个体学习器都是同一类型的学习器，即同质模式，比如都用神经网路，则称为“基学习器”（base learner），相应的学习算法称为“基学习算法”（base learning algorithm）。

在传统的机器学习中，个体学习器有时候可以是不同的，比如用决策树、支持向量机等，此时称为异质模式。

- Aggregator 结合模块

个体学习器的输出，通过一定的结合策略，在结合模块中有机结合在一起，可以形成一个能力较强的学习器，所以有时称为强学习器，而相应地称个体学习器为弱学习器。

我们只讨论使用神经网络的同质个体学习方法。个体学习器之间是否存在依赖关系呢？这取决于产生个体学习器的方法。

- Boosting系列算法，一系列的个体学习器需要一个一个地串行生成。
- Bagging算法和随机森林算（Random Forest）法，每一个个体学习器都可以独立或并行生成。

我们在此只讨论Bagging集成算法。

# Bagging 集成学习

这是Bagging集成学习的示意图：

<img src=".\Images\16\bagging.png">

首先是数据集的使用，采用自助采样法（Bootstrap Sampling）。假设原始数据集Training Set中有1000个样本，我们从中随机取一个样本的拷贝放到Training Set-1中，此时原始数据集中还有1000个样本，而不是999个，这样下次再随机取样本时，刚才那个样本还有可能被再次选到。如此方法，我们可以生成N个数据集。

然后搭建一个神经网络模型，可以是参数相同的模型，针对不同的N个数据集，训练出N个模型来，最后再进入Aggregator。N值不能太小，否则无法提供差异化的模型，也不能太大而带来训练模型的时间花销，一般来说取5到10就能满足要求。

## 生成数据集

```Python
def GenerateDataSet(count=10):
    mdr = MnistImageDataReader(train_image_file, train_label_file, test_image_file, test_label_file, "vector")
    mdr.ReadLessData(1000)
    
    for i in range(count):
        X = np.zeros_like(mdr.XTrainRaw)
        Y = np.zeros_like(mdr.YTrainRaw)
        list = np.random.choice(1000,1000)
        k=0
        for j in list:
            X[k] = mdr.XTrainRaw[j]
            Y[k] = mdr.YTrainRaw[j]
            k = k+1
        # end for
        np.savez("level6_" + str(i)+".npz", data=X, label=Y)
    # end for
```
在上面的代码中，我们假设只有1000个手写数据样本，用np.random.choice(1000,1000)函数来可重复地选取1000个数字，分别取出对应的图像数据X和标签数据Y，命名为level6_{N}.npz，N=[1,10]，保存到10个npz文件中。

假设数据集中有m个样本，这样的采样方法，某个样本在第一次被选择的概率是1/m，那么不被选择的概率就是1-1/m，则选择m次后，不被采样到的概率是$(1-\frac{1}{m})^m$，取极限值：

$$\lim_{m \rightarrow \infty} (1-\frac{1}{m})^m \simeq \frac{1}{e} = 0.368$$

即，所有样本中没有被采样的概率为36.8%。

## 训练个体学习器神经网络

这一步很简单，依然用我们在Level0中的过拟合网络，来训练10个神经网络。为了体现“弱学习器”的概念，我们可以在训练每个神经网络时，只迭代10个epoch。

```Python
    nets = []
    net_count = 10
    for i in range(net_count):
        dataReader = LoadData(i)
        net = train(dataReader)
        nets.append(net)
```
上述代码在一个10次的循环中，依次加载我们在前面生成的10个数据集，把训练好的10个net保存到一个列表中，后面测试时使用。

## 集成方法

### 平均法

在回归任务中，输出为一个数值，可以使用平均法来处理多个神经网络的输出值。
- 简单平均法：所有值加起来除以N。
  $$H(x)=\frac{1}{N} \sum_{i=1}^N h_i(x)$$

- 加权平均法：给每个输出值一个人为定义的权重。
$$H(x)=\sum_{i=1}^N w_i \cdot h_i(x)$$

权重值如何给出呢？比如第一个神经网络的准确率为80%，第二个为85%，我们可以令：

$$w1=1/0.8=1.25$$

$$w2=1/0.85=1.176$$

这样准确率高的网络会得到较大的权重值。

### 投票法

此处可以有绝对多数投票法、相对多数投票法、加权投票法等，我们只介绍一下相对多数投票法。

假设10个神经网络对于图片为数字7的预测输出为：

|ID|1|2|3|4|5|6|7|8|9|10|
|---|---|---|---|---|---|---|---|---|---|---|
|预测输出|7|4|7|4|7|7|9|7|7|1|
|标签值|正确|错误|正确|错误|正确|正确|错误|正确|正确|错误|

可以看到，在10个结果中，有6个结果正确的预测，个错误的预测，我们选择多数投票法，最终的预测结果为7，是正确的。

为了验证真实的准确率，我们可以用MNIST的测试集中的10000个样本，来测试这10个模型，得到10000个上面的表格，最后再统计最终的准确率。

此处代码比较复杂，最关键的一行语句是：

```Python
ra[i] = np.argmax(np.bincount(predict_array[:,i]))
```
先使用np.bincount得到10个神经网络的预测结果中，每个结果出现的次数，得到：

$$[0,1,0,0,2,0,0,6,0,1]$$

其含义是：数字0出现了0次，数字1出现了1次，......数字4出现了2次，......，数字7出现了6次，等等。然后再用np.argmax([0,1,0,0,2,0,0,6,0,1])得到最大的数字6的下标，结果为7。这样就可以得到10个神经网络的投票结果为该图片上的数字是7，因为有6个神经网络认为是7，占相对多数。

### 学习法

学习法，就是用另外一个神经网络，通过训练的方式，把10个神经网路的输出结果作为输入，把图片的真实数字作为标签，得到一个强学习器。

假设10个神经网络的结果如下：

|神经网络序号|1|2|3|4|5|6|7|8|9|10|标签值|
|---|---|---|---|---|---|---|---|---|---|---|---|
|预测输出1|7|4|7|4|7|7|9|7|7|1|7|
|预测输出2|4|4|7|4|7|4|9|4|7|4|4|
|预测输出N|0|9|0|0|5|0|0|6|0|1|0|
|预测输出1000|7|2|2|6|2|2|2|5|2|2|2|

还是用那1000个训练样本，得到一些列的输出，作为强学习器的输入，再训练一个分类器。

## 结果分析

|网络序号|1|2|3|4|5|6|7|8|9|10|
|---|---|---|---|---|---|---|---|---|---|---|---|
|准确率|0.8029|0.701|0.8335|0.7994|0.8254|0.8173|0.8152|0.8321|0.8546|0.8150|

10个神经网络的准确率如上表所示，最大的为0.8546，最小的为0.7010。用投票法得到的最后的准确率为0.8726，得到了提升，达到了集成学习的目的。

从偏差-方差的角度看，Bagging主要起到降低方差的作用。在前面我们分析过，单个学习器的过拟合是高方差造成的，我们训练多个这样的学习器，随机选择的样本数据如果分布均匀的话，每个学习器在针对单个测试样本时都会发生高方差现象，从而产生泛化误差。但是由于我们拥有10个神经网络，采用集成法后，一定程度上缓解了高方差的现象。


# 代码位置
ch16, Level6

# 作业

1. 实现学习法